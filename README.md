# 2023_Spring_NYCU_ML
NYCU 2023 Machine Learning

# HW1

Write a program that can do regularized linear model regression (polynomial basis) and visualization. (LSE and Newton's method)

# HW2

1. Naive Bayes classifier 

  Create a Naive Bayes classifier for each handwritten digit that support discrete and continuous

2. Online learning
   
  Use online learning to learn the beta distribution of the parameter p (chance to see 1) of the coin tossing trails in batch.

3. Prove Beta-Binomial conjugation

  Try to proof Beta-Binomial conjugation and write the process on paper.

# HW3

1. Random Data Generator

  a. Univariate gaussian data generator.

  b. Polynomial basis linear model data generator.

2. Sequential Estimator

  Sequential estimate the mean and variance.

3.Baysian Linear regression

  Use Baysian Linear regression to update the prior, and calculate the parameters of predictive distribution.

# HW4

1. Logistic regression

  Use Logistic regression to separate D1 and D2. Implement both Newton's and steepest gradient descent method during optimization.

2. EM algorithm

  Use EM algorithm to cluster each image into ten groups. Treating all pixels as random variables following Bernoulli distributions. (Training data and label sets are same as HW02)

# HW5

1. Gausian Process

  Implement the Gaussian Process and Visualize the result. Apply Gaussian Process Regression to predict the distribution of f. 
  
  Optimize the Kernel Parameters by minimizing negative marginal log-likelihood.

2. SVM on MNIST

  Use SVM models to tackle classification on images of hand-written digits.

# HW6

1. Kernel k-means

  Implement kernel k-means.

2. Spectral clustering

  Implement spectral clustering, both normalized cut and ratio cut.

# HW7

1. Kernel Eigenfaces

  Use PCA and LDA, kernel PCA and kernel LDA to do face recognition, and compute the performance.

2. t-SNE

  Modify the reference t-SNE code a little bit and make it back to symmetric SNE, and visualize the embedding and the distribution.

